# -*- coding: utf-8 -*-
"""
Created on Thu Sep 13 15:10:41 2018

@author: cruelrain
"""
# -*- coding: utf-8 -*-
"""
Created on Wed Sep  5 18:38:14 2018
@author: Lenovo
"""
 
# -*- coding: utf-8 -*-
"""
Created on Wed Sep  5 13:23:31 2018
@author: Lenovo
"""
 
import jieba.posseg as jb
import numpy as np
import lightgbm as lgb
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from gensim import corpora,models
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.similarities.docsim import Similarity
from sklearn.naive_bayes import MultinomialNB
from sklearn.cross_validation import train_test_split 
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression 
##content_id,content,subject,sentiment_value,sentiment_word
train_csv = pd.read_csv('train.csv')
test_csv = pd.read_csv('test_public.csv')
print(train_csv)
corpus = []
'''
subject
0 价格
1 配置
2 操控
3 舒适性
4 油耗
5 动力
6 内饰
7 安全性
8 空间
9 外观
'''
from keras.utils import np_utils
label = list()
elabel = list()
for i in range(len(train_csv)):
    c = train_csv.content[i]
   
    s=''
    for w in jb.cut(c):
        #print(w.word+'/'+w.flag)
        if 'a' in w.flag or 'l' in w.flag or 'v' in w.flag  or 'zg' in w.flag  or 'd' in w.flag or 'n' in w.flag:
            s=s+' '+w.word
    corpus.append(s)
    el = int(train_csv.sentiment_value[i])+1
    elabel.append(int(el))
    
    l = train_csv.subject[i]
    if l=='价格':
        label.append(0)
    if l=='配置':
        label.append(1)
    if l=='操控':
        label.append(2)
    if l=='舒适性':
        label.append(3)
    if l=='油耗':
        label.append(4)
    if l=='动力':
        label.append(5)
    if l=='内饰':
        label.append(6)
    if l=='安全性':
        label.append(7)
    if l=='空间':
        label.append(8)
    if l=='外观':
        label.append(9)
        
for i in range(len(test_csv)):
    c = test_csv.content[i]
    s=''
    for w in jb.cut(c):
        #print(w.word+'/'+w.flag)
        if 'a' in w.flag or 'l' in w.flag or 'v' in w.flag  or 'zg' in w.flag  or 'd' in w.flag or 'n' in w.flag:
            s=s+' '+w.word
    corpus.append(s)
    
#label = np_utils.to_categorical(label)
#
#cv = CountVectorizer()
#
#tfidft =TfidfTransformer()
#bow = cv.fit_transform(corpus)
stop_words=list()
stop_words.append('，')
stop_words.append('、')
stop_words.append('。')
stop_words.append('！')
stop_words.append('：')
stop_words.append('；')
stop_words.append('？')
stop_words.append('.')
stop_words.append('!')
stop_words.append(',')
stop_words.append('?')
stop_words.append(';')
 
 
vectorizer = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b",stop_words=stop_words,ngram_range=(1,2),norm='l2')
tfidf = vectorizer.fit_transform(corpus)
x_train = tfidf[0:len(label)]
x_test =tfidf[len(label):]
y_train = label
#x_train,x_test,y_train,y_test=train_test_split(tfidf[0:len(label)],label,test_size=0.25,random_state=22)
print(x_train.shape)
 
log_reg= LogisticRegression(class_weight="balanced")
log_reg.fit(x_train, y_train)
 
emo_reg= LogisticRegression(class_weight="balanced")
emo_reg.fit(x_train, elabel)
 
acc = 0
predictions = log_reg.predict_proba(x_test)
with open('car.csv','w') as f:
    for i, prediction in enumerate(predictions[:]):
        #print ('Prediction:%s. ' % (np.where(prediction>0.10)))
        ers = emo_reg.predict(x_test[i])-1
        loc = np.array(np.where(prediction>0.10))
        #print('len'+str(len(loc[0])))
        for pos in range(len(loc[0])):
            f.write(test_csv.content_id[i]+','+str(loc[0][pos])+','+str(ers[0])+','+'\n')
#    print ('Prediction:%s. Truelabel:%s.' % (prediction,y_test[i]))
#    if prediction == y_test[i]:
#        acc+=1
#print(acc/len(y_test))
